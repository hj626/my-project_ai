# legal_analyzer.py
from google import genai
import torch
import pickle
from transformers import AutoTokenizer
import json
import os

from typing import Dict, Any
from model import MultiTaskLegalBERT #ë‚´ê°€ ë§Œë“  ëª¨ë¸ ë¶ˆëŸ¬ì™€





class LegalAnalyzer:
    """ë²•ë¥  ì‚¬ê±´ ë¶„ì„ í´ë˜ìŠ¤ (BERT + Gemini)"""
    
    def __init__(self, model_path: str, gemini_api_key: str):
        """
        Args:
            model_path: í•™ìŠµëœ BERT ëª¨ë¸ ê²½ë¡œ
            gemini_api_key: Gemini API í‚¤
        """
        # BERT ëª¨ë¸ ë¡œë“œ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
        
        #ëª¨ë¸ë¡œë“œ/ë”¥ëŸ¬ë‹í–ˆë˜ ëª¨ë¸ ë¶ˆëŸ¬ì™€ 
        # ì´ê±´ ìë™í™” ì‹¤í–‰
        # self.model = MultiTaskLegalBERT.from_pretrained( #from_pretrainedí—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ëª¨ë¸ì„ ì§ê´€ì ìœ¼ë¡œê°€ì ¸ì˜¤ê²Œ í•˜ëŠ” ê±°ì•¼
        #     model_path, num_labels=3).to(self.device)
        
        # ì´ê±´ ì§ì ‘ ê°€ì„œ ë‚´ê°€ í•„ìš”í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê² ë‹¤ëŠ”ê²ƒ
        # self.model = MultiTaskLegalBERT(num_labels=3).to(self.device)
        self.model = MultiTaskLegalBERT(
        model_name="klue/bert-base",  # ğŸ‘ˆ ì—”ì§„ ì„ íƒ
        num_labels=3).to(self.device)
     
        
        model_file = os.path.join(model_path, "pytorch_model.bin")
        
        # íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ (ìƒì ê°€ì ¸ì˜¤ê¸°)
        checkpoint = torch.load(model_file,
                                map_location=self.device,
                                weights_only=False #ì´ê±´ ì•ˆì „í•œ íŒŒì¼ì´ë‹ˆê¹Œ ë³´ì•ˆ í•´ì œí•´ë„ë¼
                                )
        
        
        # 'model_state_dict'ë¼ëŠ” ì•Œë§¹ì´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ê°€ì¤‘ì¹˜ë§Œ ì¶”ì¶œ
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            print("âœ… ë”•ì…”ë„ˆë¦¬ í¬ì¥ì„ í’€ê³  ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        else:
            state_dict = checkpoint
            print("âœ… ì¼ë°˜ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        # ëª¨ë¸ ë¼ˆëŒ€ì— ì¶”ì¶œí•œ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.
        self.model.load_state_dict(state_dict)
        
        
        
        
        
        
        self.model.eval()
        
        # llm ë¶ˆëŸ¬ì™€ / Gemini ì„¤ì •
        # genai.configure(api_key=gemini_api_key)
        # self.gemini_model = genai.GenerativeModel('gemini-pro')
        self.client = genai.Client(api_key=gemini_api_key)
        self.model_name = "gemini-2.5-flash"
        
        # # í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ
        # with open(f"{model_path}/config.json", 'r') as f:
        #     config = json.load(f)
        #     self.class_names = config.get('class_names', ['ë¯¼ì‚¬/ê°€ì‚¬ì†Œì†¡', 'í–‰ì •ì†Œì†¡', 'í˜•ì‚¬ì†Œì†¡'])
    
    def predict_bert(self, text: str) -> Dict[str, Any]:
        """BERTë¡œ ê¸°ë³¸ ìˆ˜ì¹˜ ì˜ˆì¸¡"""
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=512
        )
        
         # token_type_ids ì œê±°
        inputs = {k: v.to(self.device) for k, v in inputs.items()
                  if k != 'token_type_ids'}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # # ì†Œì†¡ ìœ í˜• ì˜ˆì¸¡
        # logits = outputs['logits']
        # case_type_idx = logits.argmax(-1).item()
        
        return {
           'case_type': "ë²•ë¥  ì‚¬ê±´ ë¶„ì„", #self.class_names[case_type_idx],
            'win_rate': max(0, min(100, outputs['win_rate'].item())),
            'sentence': max(0, outputs['sentence'].item()),
            'fine': max(0, outputs['fine'].item()),
            'risk': max(0, min(100, outputs['risk'].item()))
        }
    
    def generate_feedback(self, story: str, bert_results: Dict) -> str:
        """Geminië¡œ ìƒì„¸ í”¼ë“œë°± ìƒì„±"""
        prompt = f"""
ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì´ì ìŠ¹ì†Œìœ¨ ë†’ì€ ìµœê³ ì˜ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤. ë‹¤ìŒ ì‚¬ì—°ì„ ë¶„ì„í•˜ê³  ì¡°ì–¸í•´ì£¼ì„¸ìš”.

ã€ì‚¬ì—°ã€‘
{story}

ã€AI ì˜ˆì¸¡ ê²°ê³¼ã€‘
- ì†Œì†¡ ìœ í˜•: {bert_results['case_type']}
- ìŠ¹ì†Œìœ¨: {bert_results['win_rate']:.1f}%
- ì˜ˆìƒ í˜•ëŸ‰: {bert_results['sentence']:.1f}ë…„
- ì˜ˆìƒ ë²Œê¸ˆ: {bert_results['fine']:,.0f}ì›
- ìœ„í—˜ë„: {bert_results['risk']:.1f}/100

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. ìŠ¹ì†Œìœ¨ ë¶„ì„
   - ì˜ˆì¸¡ ê·¼ê±°
   - ìœ ë¦¬í•œ ì 
   - ë¶ˆë¦¬í•œ ì 

2. ëŒ€ì‘ ì „ëµ
   - ì¦‰ì‹œ í•´ì•¼ í•  ì¡°ì¹˜
   - ì¦ê±° í™•ë³´ ë°©ì•ˆ
   - ë²•ë¥  ê²€í†  í¬ì¸íŠ¸

3. ì£¼ì˜ì‚¬í•­
   - ë²•ì  ìœ„í—˜ ìš”ì†Œ
   - í”¼í•´ì•¼ í•  í–‰ë™

4. ì „ë¬¸ê°€ ìƒë‹´ ì¶”ì²œ
   - í•„ìš”ì„± (ìƒ/ì¤‘/í•˜)
   - ì¶”ì²œ ì „ë¬¸ ë¶„ì•¼
"""
        
        # response = self.gemini_model.generate_content(prompt)
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt
        )
        return response.text
    
    def analyze(self, story: str) -> Dict[str, Any]:
        """í†µí•© ë¶„ì„ ì‹¤í–‰"""
        print("ğŸ” BERT ëª¨ë¸ ë¶„ì„ ì¤‘...")
        bert_results = self.predict_bert(story)
        
        print("ğŸ’¬ Gemini í”¼ë“œë°± ìƒì„± ì¤‘...")
        feedback = self.generate_feedback(story, bert_results)
        
        return {
            **bert_results,
            'feedback': feedback,
            'original_story': story
        }
    
    def print_result(self, result: Dict[str, Any]):
        """ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "="*70)
        print("âš–ï¸  ë²•ë¥  ì‚¬ê±´ ë¶„ì„ ê²°ê³¼")
        print("="*70)
        
        print(f"\nğŸ“‹ ì†Œì†¡ ìœ í˜•: {result['case_type']}")
        print(f"ğŸ“Š ì˜ˆìƒ ìŠ¹ì†Œìœ¨: {result['win_rate']:.1f}%")
        
        if result['sentence'] > 0.1:
            print(f"âš–ï¸  ì˜ˆìƒ í˜•ëŸ‰: {result['sentence']:.1f}ë…„")
        
        if result['fine'] > 10000:
            print(f"ğŸ’° ì˜ˆìƒ ë²Œê¸ˆ: {result['fine']:,.0f}ì›")
        
        print(f"âš ï¸  ìœ„í—˜ë„: {result['risk']:.1f}/100")
        
        print("\n" + "-"*70)
        print("ğŸ’¡ ì „ë¬¸ê°€ í”¼ë“œë°±:")
        print("-"*70)
        print(result['feedback'])
        print("="*70)