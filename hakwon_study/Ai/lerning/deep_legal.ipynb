{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ed072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë”¥ëŸ¬ë‹\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset\n",
    "from ydata_profiling import ProfileReport\n"
    "from torch.utils.data import Dataset\n",
    "from ydata_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621fb24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: í•™ìŠµìš© 48308ê±´ / í…ŒìŠ¤íŠ¸ìš© 5435ê±´\n",
      "------------------------------\n",
      "           Train(í•™ìŠµìš©) 48308  Test(í…ŒìŠ¤íŠ¸ìš©) 5435\n",
      "case_type                                   \n",
      "í˜•ì‚¬ì†Œì†¡                  37831             4542\n",
      "í–‰ì •ì†Œì†¡                   9433              801\n",
      "ë¯¼ì‚¬/ê°€ì‚¬ì†Œì†¡                1044               92\n"
     ]
    }
   ],
   "source": [
    "# 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë§Œë“¤ì–´ì§„ ë°ì´í„° ë¶ˆëŸ¬ì™€\n",
    "train_df = pd.read_pickle(\"../pkl_file/machine_data/train_machineData.pkl\")\n",
    "test_df = pd.read_pickle(\"../pkl_file/machine_data/test_machineData.pkl\")\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: í•™ìŠµìš© {len(train_df)}ê±´ / í…ŒìŠ¤íŠ¸ìš© {len(test_df)}ê±´\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ê° ë°ì´í„° í•©ê³„\n",
    "train_total = len(train_df)\n",
    "test_total = len(test_df)\n",
    "\n",
    "# ë¼ë²¨ë§ ì˜ëëŠ”ì§€ í™•ì¸\n",
    "compare_df = pd.DataFrame({\n",
    "    f'Train(í•™ìŠµìš©) {train_total}': train_df['case_type'].value_counts(),\n",
    "    f'Test(í…ŒìŠ¤íŠ¸ìš©) {test_total}': test_df['case_type'].value_counts()\n",
    "})\n",
    "print(compare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42da5ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í´ë˜ìŠ¤ ê°œìˆ˜: 3\n",
      "âœ… ê³„ì‚°ëœ ê°€ì¤‘ì¹˜: tensor([15.4240,  1.7071,  0.4256])\n",
      "['ë¯¼ì‚¬/ê°€ì‚¬ì†Œì†¡', 'í–‰ì •ì†Œì†¡', 'í˜•ì‚¬ì†Œì†¡']\n"
     ]
    }
   ],
   "source": [
    "# 2. ë¼ë²¨ ìˆ«ì ë³€í™˜\n",
    "train_df['case_type'] = train_df['case_type'].astype('category')\n",
    "test_df['case_type'] = test_df['case_type'].astype('category')\n",
    "\n",
    "num_labels = len(train_df['case_type'].cat.categories)\n",
    "train_labels = train_df['case_type'].cat.codes.tolist()\n",
    "test_labels = test_df['case_type'].cat.codes.tolist()\n",
    "\n",
    "\n",
    "# 3. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë°ì´í„° ë¶ˆê· í˜• í•´ê²°ìš©)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"âœ… í´ë˜ìŠ¤ ê°œìˆ˜: {num_labels}\")\n",
    "print(f\"âœ… ê³„ì‚°ëœ ê°€ì¤‘ì¹˜: {class_weights}\")\n",
    "# ë¯¼ì‚¬/ê°€ì‚¬ê°€ ë‚®ì•„ì„œ ê·¸ìª½ì— ê°€ì¤‘ì¹˜ë¥¼ ì¤Œ.\n",
    "# ì¹´í…Œê³ ë¦¬ ìˆœì„œ(0ë²ˆë¶€í„° ìˆœì„œëŒ€ë¡œ) ì¶œë ¥\n",
    "print(train_df['case_type'].cat.categories.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b30e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\n",
      "\n",
      " --- [ìƒ˜í”Œ ë°ì´í„° í† í¬ë‚˜ì´ì§• ê²°ê³¼] ---\n",
      "ì›ë¬¸ í…ìŠ¤íŠ¸ ì¼ë¶€: ê¸°ì†Œìœ ì˜ˆì²˜ë¶„ì·¨ì†Œ í”¼ì²­êµ¬ì¸ì´ ì²­êµ¬ì¸ì˜ ì‹ í˜¸ìœ„ë°˜ ì±…ì„ì„ ì¸ì •í•œ êµ¬ì²´ì ì¸ ì¦ê±°ë“¤ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”? í”¼ì²­êµ¬ì¸ì€ ì²­êµ¬ì¸ì—ê²Œ ì‹ í˜¸ìœ„ë°˜ì˜ ì±…ì„ì„ ì¸ì •í•˜ê¸° ìœ„í•´ ìŠ¤íƒ€ë ‰ìŠ¤ ìŠ¹í•©ì°¨ì˜ ìš´ì „ìì™€ íƒ‘ìŠ¹ìì˜...\n",
      "--------------------------------------------------\n",
      "1. ë³µì›ëœ í† í°(ì¡°ê°): \n",
      "['[CLS]', 'ê¸°ì†Œ', '##ìœ ì˜ˆ', '##ì²˜ë¶„', '##ì·¨', '##ì†Œ', 'í”¼', '##ì²­', '##êµ¬', '##ì¸', '##ì´', 'ì²­êµ¬ì¸', '##ì˜', 'ì‹ í˜¸', '##ìœ„', '##ë°˜', 'ì±…ì„', '##ì„', 'ì¸ì •', '##í•œ', 'êµ¬ì²´', '##ì ì¸', 'ì¦ê±°', '##ë“¤', '##ì€', 'ë¬´ì—‡', '##ì´', '##ì—ˆ', '##ë‚˜', '##ìš”'] ...\n",
      "--------------------------------------------------\n",
      "2. ì¸ì½”ë”©ëœ ìˆ«ì(ID): \n",
      "[2, 5927, 13145, 12892, 2586, 2024, 1882, 2270, 2251, 2179, 2052, 21718, 2079, 6320, 2090, 2536, 3998, 2069, 4089, 2470, 4468, 31221, 5331, 2031, 2073, 3890, 2052, 2359, 2075, 2182] ...\n"
     ]
    }
   ],
   "source": [
    "# 4. í† í°í™” (í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ìª¼ê°¬. ë©ˆì¶¤ ë°©ì§€ë¥¼ ìœ„í•´ max_lengthë¥¼ 512 ì„¤ì •)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, \n",
    "                            padding='max_length', max_length=512)\n",
    "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True,\n",
    "                           padding='max_length', max_length=512)\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")\n",
    "\n",
    "# [í† í¬ë‚˜ì´ì§• ìƒ˜í”Œ í™•ì¸]\n",
    "sample_index = 0  # ì²« ë²ˆì§¸ ë°ì´í„° í™•ì¸\n",
    "\n",
    "# 1. ìˆ«ìë¡œ ë³€í™˜ëœ ê²°ê³¼ (input_ids)\n",
    "sample_ids = train_encodings['input_ids'][sample_index]\n",
    "\n",
    "# 2. ìˆ«ìë¥¼ ë‹¤ì‹œ ë‹¨ì–´ ì¡°ê°(Tokens)ìœ¼ë¡œ ë³µì›\n",
    "sample_tokens = tokenizer.convert_ids_to_tokens(sample_ids)\n",
    "\n",
    "print(f\"\\n --- [ìƒ˜í”Œ ë°ì´í„° í† í¬ë‚˜ì´ì§• ê²°ê³¼] ---\")\n",
    "print(f\"ì›ë¬¸ í…ìŠ¤íŠ¸ ì¼ë¶€: {train_df['text'].iloc[sample_index][:100]}...\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"1. ë³µì›ëœ í† í°(ì¡°ê°): \\n{sample_tokens[:30]} ...\") \n",
    "print(\"-\" * 50)\n",
    "print(f\"2. ì¸ì½”ë”©ëœ ìˆ«ì(ID): \\n{sample_ids[:30]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd63389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- í…ì„œ ë³€í™˜ ê²°ê³¼ í™•ì¸ ---\n",
      "ğŸ”¹ í•„ë“œëª…: input_ids\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([512])\n",
      "   - ì‹¤ì œ ê°’(ì¼ë¶€): [2, 5927, 13145, 12892, 2586, 2024, 1882, 2270, 2251, 2179]...\n",
      "------------------------------\n",
      "ğŸ”¹ í•„ë“œëª…: token_type_ids\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([512])\n",
      "   - ì‹¤ì œ ê°’(ì¼ë¶€): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "------------------------------\n",
      "ğŸ”¹ í•„ë“œëª…: attention_mask\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([512])\n",
      "   - ì‹¤ì œ ê°’(ì¼ë¶€): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "------------------------------\n",
      "ğŸ”¹ í•„ë“œëª…: labels\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([])\n",
      "   - ì‹¤ì œ ê°’: 2\n",
      "------------------------------\n",
      "ğŸ”¹ í•„ë“œëª…: win_rate\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([])\n",
      "   - ì‹¤ì œ ê°’: 50.0\n",
      "------------------------------\n",
      "ğŸ”¹ í•„ë“œëª…: sentence\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([])\n",
      "   - ì‹¤ì œ ê°’: 0.0\n",
      "------------------------------\n",
      "ğŸ”¹ í•„ë“œëª…: fine\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([])\n",
      "   - ì‹¤ì œ ê°’: 0.0\n",
      "------------------------------\n",
      "ğŸ”¹ í•„ë“œëª…: risk\n",
      "   - ë°ì´í„° íƒ€ì…: <class 'torch.Tensor'>\n",
      "   - ëª¨ì–‘(Shape): torch.Size([])\n",
      "   - ì‹¤ì œ ê°’: 10.0\n",
      "------------------------------\n",
      "âœ… ì‹¤ì œ ëª¨ë¸ì´ ì½ëŠ” í…ìŠ¤íŠ¸(í•´ì„): ê¸°ì†Œìœ ì˜ˆì²˜ë¶„ì·¨ì†Œ í”¼ì²­êµ¬ì¸ì´ ì²­êµ¬ì¸ì˜ ì‹ í˜¸ìœ„ë°˜ ì±…ì„ì„ ì¸ì •í•œ êµ¬ì²´ì ì¸ ì¦ê±°ë“¤ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”? í”¼ì²­êµ¬ì¸ì€ ì²­êµ¬ì¸ì—ê²Œ ì‹ í˜¸ìœ„ë°˜ì˜ ì±…ì„ì„ ì¸ì •í•˜ê¸° ìœ„í•´ ìŠ¤íƒ€ë ‰ìŠ¤ ìŠ¹í•©ì°¨ì˜ ìš´ì „ìì™€ íƒ‘ìŠ¹ìì˜...\n"
     ]
    }
   ],
   "source": [
    "# 5. Dataset ë° WeightedTrainer ì •ì˜ (ì‚¬ìš©ì ê¸°ì¡´ ì½”ë“œ ìœ ì§€)\n",
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, win_rates, sentences, fines, risks):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.win_rates = win_rates\n",
    "        self.sentences = sentences\n",
    "        self.fines = fines\n",
    "        self.risks = risks\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # í•™ìŠµ ì‹œ í•˜ë‚˜ì”© êº¼ë‚´ì„œ í…ì„œ(Tensor/ìˆ«ìë¥¼ ë‹´ëŠ” ë‹¤ì°¨ì› ìƒì) í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ì—­í• \n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # labelsëŠ” ë‚´ë¶€ ë¡œì§ìƒ í•„ìš”í•˜ë¯€ë¡œ ìœ ì§€í•˜ë˜, ë‚˜ì¤‘ì— ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¹„ì¤‘ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['win_rate'] = torch.tensor(self.win_rates[idx], dtype=torch.float)\n",
    "        item['sentence'] = torch.tensor(self.sentences[idx], dtype=torch.float)\n",
    "        item['fine'] = torch.tensor(self.fines[idx], dtype=torch.float)\n",
    "        item['risk'] = torch.tensor(self.risks[idx], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±(ë°ì´í„° ê°€ì ¸ì™€)\n",
    "# ë°ì´í„°ì…‹ ê°ì²´ ìƒì„± ì‹œ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë§Œë“  ì»¬ëŸ¼ë„£ê¸°\n",
    "train_dataset = LegalDataset(\n",
    "    train_encodings, train_labels,\n",
    "    train_df['win_rate'].values, \n",
    "    train_df['sentence_years'].values,\n",
    "    train_df['fine_amount'].values, \n",
    "    train_df['risk_score'].values\n",
    ")\n",
    "test_dataset = LegalDataset(\n",
    "    test_encodings, test_labels,\n",
    "    test_df['win_rate'].values, \n",
    "    test_df['sentence_years'].values,\n",
    "    test_df['fine_amount'].values, \n",
    "    test_df['risk_score'].values\n",
    ")\n",
    "\n",
    "\n",
    "# í…ì„œ í˜•íƒœ ë³€í™˜í•œê±° í™•ì¸í•˜ê¸°!\n",
    "# í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ ì²« ë²ˆì§¸ ìƒ˜í”Œ(ì¸ë±ìŠ¤ 0) í•˜ë‚˜ë¥¼ êº¼ë‚´ë´…ë‹ˆë‹¤.\n",
    "sample_item = train_dataset[0]\n",
    "\n",
    "print(\"--- í…ì„œ ë³€í™˜ ê²°ê³¼ í™•ì¸ ---\")\n",
    "for key, value in sample_item.items():\n",
    "    print(f\"ğŸ”¹ í•„ë“œëª…: {key}\")\n",
    "    print(f\"   - ë°ì´í„° íƒ€ì…: {type(value)}\") # torch.Tensorì¸ì§€ í™•ì¸\n",
    "    print(f\"   - ëª¨ì–‘(Shape): {value.shape}\")\n",
    "    \n",
    "    # ë°ì´í„°ê°€ 0ì°¨ì›(ìŠ¤ì¹¼ë¼-> í•œê°œì˜ ìˆ«ì)ì¸ì§€ í™•ì¸í•˜ì—¬ ì¶œë ¥ ë°©ì‹ ë³€ê²½\n",
    "    if value.dim() == 0:\n",
    "        print(f\"   - ì‹¤ì œ ê°’: {value.item()}\") # ìˆ«ì í•˜ë‚˜ë§Œ ì¶œë ¥\n",
    "    else:\n",
    "        print(f\"   - ì‹¤ì œ ê°’(ì¼ë¶€): {value[:10].tolist()}...\")# ìŠ¬ë¼ì´ì‹± í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# ë””ì½”ë”© í™•ì¸\n",
    "decoded_text = tokenizer.decode(sample_item['input_ids'], skip_special_tokens=True)\n",
    "print(f\"âœ… ì‹¤ì œ ëª¨ë¸ì´ ì½ëŠ” í…ìŠ¤íŠ¸(í•´ì„): {decoded_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e0c239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ê³„ë„ ì™„ì„± ë° ì‹¤ì œ ë°ì´í„°ì…‹(train/test) ìƒì„± ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤(ê°€ì¤‘ì¹˜ ê³„ì‚° ì„¤ê³„ë„)\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        # logits = outputs.get(\"logits\")\n",
    "        loss = outputs.get(\"loss\")\n",
    "        \n",
    "        # ë°ì´í„° ë¶ˆê· í˜•ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë¯¸ë¦¬ ë§Œë“  class_weights ì ìš©\n",
    "        # loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        # loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"ì„¤ê³„ë„ ì™„ì„± ë° ì‹¤ì œ ë°ì´í„°ì…‹(train/test) ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a128e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- âš–ï¸ WeightedTrainerê°€ ì‚¬ìš©í•  ê°€ì¤‘ì¹˜ ì„¤ê³„ë„ ---\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì¤‘ì¹˜ ì„¤ê³„ë„ í™•ì¸\n",
    "\n",
    "# 1. ë¼ë²¨ ì´ë¦„ ëª©ë¡ ì¤€ë¹„\n",
    "if 'case_type' in train_df.columns and hasattr(train_df['case_type'], 'cat'):\n",
    "    class_names = train_df['case_type'].cat.categories.tolist()\n",
    "    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì—ëŸ¬ ë°©ì§€\n",
    "    \n",
    "else:\n",
    "    class_names = [f\"Class_{i}\" for i in range(num_labels)]\n",
    "\n",
    "print(\"--- âš–ï¸ WeightedTrainerê°€ ì‚¬ìš©í•  ê°€ì¤‘ì¹˜ ì„¤ê³„ë„ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c886c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ë¼ë²¨ 0 [ë¯¼ì‚¬/ê°€ì‚¬ì†Œì†¡        ] ê°€ì¤‘ì¹˜: 15.4240\n",
      "ğŸ“ ë¼ë²¨ 1 [í–‰ì •ì†Œì†¡           ] ê°€ì¤‘ì¹˜: 1.7071\n",
      "ğŸ“ ë¼ë²¨ 2 [í˜•ì‚¬ì†Œì†¡           ] ê°€ì¤‘ì¹˜: 0.4256\n",
      "\n",
      "ğŸ’¡ ê°€ì¤‘ì¹˜ í•´ì„ ê°€ì´ë“œ:\n",
      "âœ… ê°€ì¤‘ì¹˜ê°€ ê°€ì¥ ë†’ì€ ê²ƒ: ë¯¼ì‚¬/ê°€ì‚¬ì†Œì†¡ (ë°ì´í„°ê°€ ê°€ì¥ ì ìŒ -> ì§‘ì¤‘ í•™ìŠµ)\n",
      "âœ… ê°€ì¤‘ì¹˜ê°€ ê°€ì¥ ë‚®ì€ ê²ƒ: í˜•ì‚¬ì†Œì†¡ (ë°ì´í„°ê°€ ê°€ì¥ ë§ìŒ -> ì¼ë°˜ í•™ìŠµ)\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì¤‘ì¹˜ê°’ ì¶œë ¥\n",
    "weights_list = class_weights.tolist()\n",
    "for i, weight in enumerate(weights_list):\n",
    "    name = class_names[i]\n",
    "    print(f\"ğŸ“ ë¼ë²¨ {i} [{name:15s}] ê°€ì¤‘ì¹˜: {weight:.4f}\")\n",
    "\n",
    "# 2. ê°€ì¤‘ì¹˜ í•´ì„ ê°€ì´ë“œ\n",
    "# .argmax() ë’¤ì— .item()ì„ ë¶™ì—¬ì„œ 'ìˆœìˆ˜í•œ ì¸ë±ìŠ¤ ìˆ«ì'ë§Œ ì¶”ì¶œ.\n",
    "max_idx = class_weights.argmax().item()\n",
    "min_idx = class_weights.argmin().item()\n",
    "\n",
    "print(\"\\nğŸ’¡ ê°€ì¤‘ì¹˜ í•´ì„ ê°€ì´ë“œ:\")\n",
    "print(f\"âœ… ê°€ì¤‘ì¹˜ê°€ ê°€ì¥ ë†’ì€ ê²ƒ: {class_names[max_idx]} (ë°ì´í„°ê°€ ê°€ì¥ ì ìŒ -> ì§‘ì¤‘ í•™ìŠµ)\")\n",
    "print(f\"âœ… ê°€ì¤‘ì¹˜ê°€ ê°€ì¥ ë‚®ì€ ê²ƒ: {class_names[min_idx]} (ë°ì´í„°ê°€ ê°€ì¥ ë§ìŒ -> ì¼ë°˜ í•™ìŠµ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdab544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±„ì  í•¨ìˆ˜ ì •ì˜ (Metrics)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    # predictionsê°€ íŠœí”Œì¸ ê²½ìš° logitsë§Œ ì¶”ì¶œ\n",
    "    if isinstance(pred.predictions, tuple): #ë°ì´í„° íƒ€ì…ì´ íŠœí”Œì¸ì§€ í™•ì¸í•´ë´\n",
    "        logits = pred.predictions[-1]  # ë§ˆì§€ë§‰ ìš”ì†Œê°€ logits\n",
    "    else:\n",
    "        logits = pred.predictions\n",
    "        \n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "    preds = logits.argmax(-1)\n",
    "     \n",
    "    # predictionsê°€ íŠœí”Œì¸ ê²½ìš° logitsë§Œ ì¶”ì¶œ\n",
    "    if isinstance(pred.predictions, tuple): #ë°ì´í„° íƒ€ì…ì´ íŠœí”Œì¸ì§€ í™•ì¸í•´ë´\n",
    "        logits = pred.predictions[-1]  # ë§ˆì§€ë§‰ ìš”ì†Œê°€ logits\n",
    "    else:\n",
    "        logits = pred.predictions\n",
    "        \n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "    preds = logits.argmax(-1)\n",
    "     \n",
    "    # ì •í™•ë„(Accuracy)ì™€ ë¶ˆê· í˜•ì„ ê³ ë ¤í•œ F1-score ê³„ì‚°\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro') \n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa51f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=num_labels).to(device)\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class MultiTaskLegalBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.config = self.bert.config\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # ìˆ˜ì¹˜ ì˜ˆì¸¡ìš© 4ê°€ì§€ í—¤ë“œ (Regression)\n",
    "        self.win_rate_head = nn.Linear(hidden_size, 1)\n",
    "        self.sentence_head = nn.Linear(hidden_size, 1)\n",
    "        self.fine_head = nn.Linear(hidden_size, 1)\n",
    "        self.risk_head = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # ì†Œì†¡ ë¶„ë¥˜ëŠ” ë‚´ë¶€ í•™ìŠµìš©ìœ¼ë¡œë§Œ ìœ ì§€ (BERTê°€ ë²•ë¥  ë§¥ë½ì„ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì„ ì¤Œ)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, win_rate=None, sentence=None, fine=None, risk=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output # CLS í† í°ì˜ ê²°ê³¼ë¬¼\n",
    "        \n",
    "        # ê° ìˆ˜ì¹˜ ì˜ˆì¸¡\n",
    "        pred_win = self.win_rate_head(pooled_output).squeeze(-1)\n",
    "        pred_sent = self.sentence_head(pooled_output).squeeze(-1)\n",
    "        pred_fine = self.fine_head(pooled_output).squeeze(-1)\n",
    "        pred_risk = self.risk_head(pooled_output).squeeze(-1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if win_rate is not None:\n",
    "            mse_fct = nn.MSELoss()\n",
    "            # 4ê°€ì§€ ìˆ˜ì¹˜ì— ëŒ€í•œ ì˜¤ì°¨ í•©ì‚°\n",
    "            loss = mse_fct(pred_win, win_rate) + \\\n",
    "                   mse_fct(pred_sent, sentence) + \\\n",
    "                   mse_fct(pred_fine, fine) + \\\n",
    "                   mse_fct(pred_risk, risk)\n",
    "            \n",
    "            # ì†Œì†¡ ìœ í˜• ë¶„ë¥˜ ì†ì‹¤ë„ ì¡°ê¸ˆ ì¶”ê°€í•´ì„œ í•™ìŠµì„ ë„ì›€ (ê°€ì¤‘ì¹˜ 0.1)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss += 0.1 * loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return {\"loss\": loss, \"win_rate\": pred_win, \"sentence\": pred_sent, \"fine\": pred_fine, \"risk\": pred_risk, \"logits\": logits}\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MultiTaskLegalBERT(\"klue/bert-base\", num_labels=num_labels).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì°¨ì§€í•´ì„œ ì»¤ë„ì¶©ëŒì´ ë°œìƒí•¨. -> ì•„ë˜ ë‘ê°œ ì½”ë“œë¡œ \n",
    "# ë©”ëª¨ë¦¬(RAM)ë¶€ì¡± ë°©ì§€/í•„ìš”í• ë•Œ ë‹¤ì‹œ ê³„ì‚°í•´ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì„\n",
    "# model ë‚´ë¶€ì˜ bert ëª¨ë¸ì— ìˆëŠ” ë©”ì„œë“œë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "model.bert.gradient_checkpointing_enable()\n",
    "model.to(device)\n",
    "\n",
    "print(\"ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90aef949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import psutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0979f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n",
    "def create_sample_dataset(dataset, sample_ratio=0.1):\n",
    "    \"\"\"ë°ì´í„°ì˜ ì¼ë¶€ë§Œ ì¶”ì¶œ\"\"\"\n",
    "    total_size = len(dataset)\n",
    "    sample_size = int(total_size * sample_ratio)\n",
    "    indices = random.sample(range(total_size), sample_size)\n",
    "    return Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928e39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ í™•ì¸ í•¨ìˆ˜\n",
    "def check_memory_before_training():\n",
    "    mem = psutil.virtual_memory()\n",
    "    available_gb = mem.available / (1024**3)\n",
    "    \n",
    "    print(f\"ğŸ’¾ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {available_gb:.1f} GB\")\n",
    "    \n",
    "    if available_gb < 2.0:\n",
    "        print(\"âš ï¸ ê²½ê³ : ë©”ëª¨ë¦¬ ë¶€ì¡±! ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ì„¸ìš”.\")\n",
    "        return False\n",
    "    elif available_gb < 4.0:\n",
    "        print(\"âš ï¸ ì£¼ì˜: ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ ì ìŠµë‹ˆë‹¤.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âœ… ë©”ëª¨ë¦¬ ì¶©ë¶„í•©ë‹ˆë‹¤.\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe88144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸\n",
    "MODE_1_args = TrainingArguments(\n",
    "    output_dir=\"./mode1_quick_test\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    max_steps=10,  # 10 ìŠ¤í…ë§Œ!\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=2,\n",
    "    save_steps=10,\n",
    "    \n",
    "    fp16=False,  # CPUëŠ” False\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3c5b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ì¦\n",
    "MODE_2_args = TrainingArguments(\n",
    "    output_dir=\"./mode2_quick_validation\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154c3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ìš© ëª¨ë¸\n",
    "MODE_3_args = TrainingArguments(\n",
    "    output_dir=\"./mode3_practical_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7625e48d",
   "id": "90aef949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import psutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0979f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n",
    "def create_sample_dataset(dataset, sample_ratio=0.1):\n",
    "    \"\"\"ë°ì´í„°ì˜ ì¼ë¶€ë§Œ ì¶”ì¶œ\"\"\"\n",
    "    total_size = len(dataset)\n",
    "    sample_size = int(total_size * sample_ratio)\n",
    "    indices = random.sample(range(total_size), sample_size)\n",
    "    return Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928e39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ í™•ì¸ í•¨ìˆ˜\n",
    "def check_memory_before_training():\n",
    "    mem = psutil.virtual_memory()\n",
    "    available_gb = mem.available / (1024**3)\n",
    "    \n",
    "    print(f\"ğŸ’¾ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {available_gb:.1f} GB\")\n",
    "    \n",
    "    if available_gb < 2.0:\n",
    "        print(\"âš ï¸ ê²½ê³ : ë©”ëª¨ë¦¬ ë¶€ì¡±! ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ì„¸ìš”.\")\n",
    "        return False\n",
    "    elif available_gb < 4.0:\n",
    "        print(\"âš ï¸ ì£¼ì˜: ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ ì ìŠµë‹ˆë‹¤.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âœ… ë©”ëª¨ë¦¬ ì¶©ë¶„í•©ë‹ˆë‹¤.\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe88144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸\n",
    "MODE_1_args = TrainingArguments(\n",
    "    output_dir=\"./mode1_quick_test\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    max_steps=10,  # 10 ìŠ¤í…ë§Œ!\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=2,\n",
    "    save_steps=10,\n",
    "    \n",
    "    fp16=False,  # CPUëŠ” False\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3c5b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ì¦\n",
    "MODE_2_args = TrainingArguments(\n",
    "    output_dir=\"./mode2_quick_validation\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154c3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ìš© ëª¨ë¸\n",
    "MODE_3_args = TrainingArguments(\n",
    "    output_dir=\"./mode3_practical_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7625e48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "ğŸ’¾ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: 6.0 GB\n",
      "âœ… ë©”ëª¨ë¦¬ ì¶©ë¶„í•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š í•™ìŠµ ì„¤ì •\n",
      "  - í•™ìŠµ ë°ì´í„°: 48,308ê°œ\n",
      "  - í‰ê°€ ë°ì´í„°: 5,435ê°œ\n",
      "\n",
      " í•™ìŠµ ì‹œì‘!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 1:42:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4562498355200.000000</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.250414</td>\n",
       "      <td>0.163803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1124998578176.000000</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.106164</td>\n",
       "      <td>0.111891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "â±ï¸  ì†Œìš” ì‹œê°„: 1ì‹œê°„ 42ë¶„\n"
     ]
    }
   ],
   "source": [
    "print(\"í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "\n",
    "# ëª¨ë“œ1 ë°ì´í„° 100% ì‚¬ìš©\n",
    "# ë©”ëª¨ë¦¬ ì²´í¬\n",
    "if not check_memory_before_training():\n",
    "    raise RuntimeError(\"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±!\")\n",
    "\n",
    "# í•™ìŠµì‹¤í–‰í•¨ìˆ˜\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ì„¤ì •\")\n",
    "print(f\"  - í•™ìŠµ ë°ì´í„°: {len(train_dataset):,}ê°œ\")\n",
    "print(f\"  - í‰ê°€ ë°ì´í„°: {len(test_dataset):,}ê°œ\")\n",
    "\n",
    "# íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=MODE_1_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "\n",
    "# ëª¨ë“œ1 ë°ì´í„° 100% ì‚¬ìš©\n",
    "# ë©”ëª¨ë¦¬ ì²´í¬\n",
    "if not check_memory_before_training():\n",
    "    raise RuntimeError(\"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±!\")\n",
    "\n",
    "# í•™ìŠµì‹¤í–‰í•¨ìˆ˜\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ì„¤ì •\")\n",
    "print(f\"  - í•™ìŠµ ë°ì´í„°: {len(train_dataset):,}ê°œ\")\n",
    "print(f\"  - í‰ê°€ ë°ì´í„°: {len(test_dataset):,}ê°œ\")\n",
    "\n",
    "# íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=MODE_1_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "print(f\"\\n í•™ìŠµ ì‹œì‘!\\n\")\n",
    "\n",
    "# í•™ìŠµì‹¤í–‰\n",
    "import time\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "\n",
    "# ì†Œìš” ì‹œê°„ ê³„ì‚°\n",
    "elapsed = time.time() - start_time\n",
    "hours = int(elapsed // 3600)\n",
    "minutes = int((elapsed % 3600) // 60)\n",
    "\n",
    "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"â±ï¸  ì†Œìš” ì‹œê°„: {hours}ì‹œê°„ {minutes}ë¶„\")"
    "print(f\"\\n í•™ìŠµ ì‹œì‘!\\n\")\n",
    "\n",
    "# í•™ìŠµì‹¤í–‰\n",
    "import time\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "\n",
    "# ì†Œìš” ì‹œê°„ ê³„ì‚°\n",
    "elapsed = time.time() - start_time\n",
    "hours = int(elapsed // 3600)\n",
    "minutes = int((elapsed % 3600) // 60)\n",
    "\n",
    "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"â±ï¸  ì†Œìš” ì‹œê°„: {hours}ì‹œê°„ {minutes}ë¶„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17ac7850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...\n"
      "\n",
      "ğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='680' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [680/680 46:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ [í‰ê°€ ê²°ê³¼]\n",
      "  eval_loss: 115086031060992.0000\n",
      "  eval_accuracy: 0.1062\n",
      "  eval_f1: 0.1119\n",
      "  eval_runtime: 2820.6488\n",
      "  eval_samples_per_second: 1.9270\n",
      "  eval_steps_per_second: 0.2410\n",
      "  epoch: 0.0017\n",
      "\n",
      "ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: ./saved_mode1\n",
      "âœ… ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€\n",
    "print(f\"\\nğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nğŸ¯ [í‰ê°€ ê²°ê³¼]\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "# save_path = f\"./saved_{mode.lower()}\"\n",
    "save_path = \"./saved_mode1\"\n",
    "print(f\"\\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {save_path}\")\n",
    "\n",
    "# ì¶”ê°€\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë¸ì´ë¯€ë¡œ torch.save ì‚¬ìš©\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# PyTorch ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ì €ì¥\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'num_labels': num_labels,\n",
    "        'class_names': train_df['case_type'].cat.categories.tolist(),\n",
    "        'class_weights': class_weights.cpu().numpy()\n",
    "    }\n",
    "}, os.path.join(save_path, 'pytorch_model.bin'))\n",
    "\n",
    "# model.save_pretrained(save_path)     \n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b40745d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•™ìŠµ ê²°ê³¼ê°€ train_deep_data.pklì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ì €ì¥\n",
    "import pickle\n",
    "\n",
    "results_to_save = {\n",
    "    'eval_results': eval_results,\n",
    "    'model_config': {\n",
    "        'num_labels': num_labels,\n",
    "        'class_names': train_df['case_type'].cat.categories.tolist(),\n",
    "        'class_weights': class_weights.cpu().numpy()\n",
    "    },\n",
    "    'training_history': trainer.state.log_history\n",
    "}\n",
    "\n",
    "with open('../pkl_file/deep_data/train_deep_data.pkl', 'wb') as f:\n",
    "    pickle.dump(results_to_save, f)\n",
    "\n",
    "print(\"âœ… í•™ìŠµ ê²°ê³¼ê°€ train_deep_data.pklì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fb706cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸƒ MODE 2 ì‹¤í–‰\n",
      "======================================================================\n",
      "ğŸ’¾ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: 4.8 GB\n",
      "âœ… ë©”ëª¨ë¦¬ ì¶©ë¶„í•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š í•™ìŠµ ì„¤ì •\n",
      "  - í•™ìŠµ ë°ì´í„°: 2,415ê°œ (5% ìƒ˜í”Œ)\n",
      "ìƒ˜í”Œë§: 5.0%\n",
      "  - í‰ê°€ ë°ì´í„°: 5,435ê°œ\n"
     ]
    }
   ],
   "source": [
    "# 20ë²ˆ ì…€ - MODE_2 ì‹¤í–‰\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸƒ MODE 2 ì‹¤í–‰\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì²´í¬\n",
    "if not check_memory_before_training():\n",
    "    raise RuntimeError(\"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±!\")\n",
    "\n",
    "# MODE_2 ì„¤ì •\n",
    "train_data_2 = create_sample_dataset(train_dataset, 0.05) #ë°ì´í„° 5%ë§Œ ì‚¬ìš©\n",
    "\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ì„¤ì •\")\n",
    "print(f\"  - í•™ìŠµ ë°ì´í„°: {len(train_data_2):,}ê°œ (5% ìƒ˜í”Œ)\")\n",
    "print(f\"ìƒ˜í”Œë§: {0.05*100}%\")\n",
    "print(f\"  - í‰ê°€ ë°ì´í„°: {len(test_dataset):,}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937a3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MODE_2 í•™ìŠµ ì‹œì‘!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 5:21:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>845311010406.400024</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.024287</td>\n",
       "      <td>0.040199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>563507815985971.250000</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.045262</td>\n",
       "      <td>0.032721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1700182307635.199951</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.077461</td>\n",
       "      <td>0.050725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1373807994470.399902</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.112787</td>\n",
       "      <td>0.069682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í•™ìŠµ ì™„ë£Œ! â±ï¸ 5ì‹œê°„ 22ë¶„\n"
     ]
    }
   ],
   "source": [
    "# íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "trainer_2 = WeightedTrainer(\n",
    "# íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "trainer_2 = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=MODE_2_args,\n",
    "    train_dataset=train_data_2,\n",
    "    args=MODE_2_args,\n",
    "    train_dataset=train_data_2,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(f\"\\n MODE_2 í•™ìŠµ ì‹œì‘!\\n\")\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "start_time_2 = time.time()\n",
    "trainer_2.train()\n",
    "\n",
    "# ì†Œìš” ì‹œê°„\n",
    "elapsed_2 = time.time() - start_time_2\n",
    "hours = int(elapsed_2 // 3600)\n",
    "minutes = int((elapsed_2 % 3600) // 60)\n",
    "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ! â±ï¸ {hours}ì‹œê°„ {minutes}ë¶„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddaaf965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='680' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [680/680 50:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ MODE_2 í‰ê°€ ê²°ê³¼:\n",
      "  eval_loss: 115086031060992.0000\n",
      "  eval_accuracy: 0.1128\n",
      "  eval_f1: 0.0697\n",
      "  eval_runtime: 3033.3189\n",
      "  eval_samples_per_second: 1.7920\n",
      "  eval_steps_per_second: 0.2240\n",
      "  epoch: 0.6625\n",
      "âœ… MODE_2 ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ ë° ì €ì¥\n",
    "eval_results_2 = trainer_2.evaluate()\n",
    "print(f\"\\nğŸ¯ MODE_2 í‰ê°€ ê²°ê³¼:\")\n",
    "for key, value in eval_results_2.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "save_path_2 = \"./saved_mode2\"\n",
    "os.makedirs(save_path_2, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'num_labels': num_labels,\n",
    "        'class_names': train_df['case_type'].cat.categories.tolist(),\n",
    "        'class_weights': class_weights.cpu().numpy()\n",
    "    }\n",
    "}, os.path.join(save_path_2, 'pytorch_model.bin'))\n",
    "\n",
    "tokenizer.save_pretrained(save_path_2)\n",
    "print(\"âœ… MODE_2 ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bf855b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MODE_2 ì™„ë£Œ ë° ì €ì¥!\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ì €ì¥\n",
    "results_2 = {\n",
    "    'eval_results': eval_results_2,\n",
    "    'model_config': {\n",
    "        'num_labels': num_labels,\n",
    "        'class_names': train_df['case_type'].cat.categories.tolist(),\n",
    "        'class_weights': class_weights.cpu().numpy()\n",
    "    },\n",
    "    'training_history': trainer_2.state.log_history,\n",
    "    'elapsed_time': elapsed_2\n",
    "}\n",
    "\n",
    "with open('../pkl_file/deep_data/train_deep_data_model2.pkl', 'wb') as f:\n",
    "    pickle.dump(results_2, f)\n",
    "\n",
    "print(\"âœ… MODE_2 ì™„ë£Œ ë° ì €ì¥!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e92f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MODE 3: ì‹¤ìš© ëª¨ë¸ í•™ìŠµ\n",
      "======================================================================\n",
      "ğŸ’¾ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: 6.3 GB\n",
      "âœ… ë©”ëª¨ë¦¬ ì¶©ë¶„í•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š í•™ìŠµ ì„¤ì •\n",
      "  - í•™ìŠµ ë°ì´í„°: 4,830ê°œ\n",
      "ìƒ˜í”Œë§: 10.0%\n",
      "  - í‰ê°€ ë°ì´í„°: 5,435ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ìƒˆ ì…€ - MODE_3 ì‹¤í–‰\n",
    "print(\"\\n MODE 3: ì‹¤ìš© ëª¨ë¸ í•™ìŠµ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì²´í¬\n",
    "if not check_memory_before_training():\n",
    "    raise RuntimeError(\"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±!\")\n",
    "\n",
    "# MODE_3 ì„¤ì • (10% ìƒ˜í”Œ)\n",
    "train_data_3 = create_sample_dataset(train_dataset, 0.10)\n",
    "sample_ratio_3 = 0.10\n",
    "\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ì„¤ì •\")\n",
    "print(f\"  - í•™ìŠµ ë°ì´í„°: {len(train_data_3):,}ê°œ\")\n",
    "print(f\"ìƒ˜í”Œë§: {sample_ratio_3*100}%\")  # 10.0% ì¶œë ¥\n",
    "print(f\"  - í‰ê°€ ë°ì´í„°: {len(test_dataset):,}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30a83e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MODE_3 í•™ìŠµ ì‹œì‘!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='302' max='302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [302/302 9:00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7032402497817805.000000</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.882981</td>\n",
       "      <td>0.541035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1214805599846.399902</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.835695</td>\n",
       "      <td>0.303498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>9369767274086.400391</td>\n",
       "      <td>115086031060992.000000</td>\n",
       "      <td>0.835695</td>\n",
       "      <td>0.303498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í•™ìŠµ ì™„ë£Œ! â±ï¸ 9ì‹œê°„ 1ë¶„\n"
     ]
    }
   ],
   "source": [
    "# íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "trainer_3 = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=MODE_3_args,  # ë” ë§ì€ ìŠ¤í…, warmup ë“±\n",
    "    train_dataset=train_data_3,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(f\"\\n MODE_3 í•™ìŠµ ì‹œì‘!\\n\")\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "start_time_3 = time.time()\n",
    "trainer_3.train()\n",
    "\n",
    "# ì†Œìš” ì‹œê°„\n",
    "elapsed_3 = time.time() - start_time_3\n",
    "hours = int(elapsed_3 // 3600)\n",
    "minutes = int((elapsed_3 % 3600) // 60)\n",
    "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ! â±ï¸ {hours}ì‹œê°„ {minutes}ë¶„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='680' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [680/680 48:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ MODE_3 í‰ê°€ ê²°ê³¼:\n",
      "  eval_loss: 115086031060992.0000\n",
      "  eval_accuracy: 0.8357\n",
      "  eval_f1: 0.3035\n",
      "  eval_runtime: 2902.2379\n",
      "  eval_samples_per_second: 1.8730\n",
      "  eval_steps_per_second: 0.2340\n",
      "  epoch: 1.0000\n",
      "ëª¨ë¸ ì €ì¥ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€\n",
    "eval_results_3 = trainer_3.evaluate()\n",
    "print(f\"\\nğŸ¯ MODE_3 í‰ê°€ ê²°ê³¼:\")\n",
    "for key, value in eval_results_3.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "save_path_3 = \"./saved_mode3\"\n",
    "os.makedirs(save_path_3, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'num_labels': num_labels,\n",
    "        'class_names': train_df['case_type'].cat.categories.tolist(),\n",
    "        'class_weights': class_weights.cpu().numpy()\n",
    "    }\n",
    "}, os.path.join(save_path_3, 'pytorch_model.bin'))\n",
    "\n",
    "tokenizer.save_pretrained(save_path_3)\n",
    "print(\"ëª¨ë¸ ì €ì¥ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ae7a6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MODE_3 ì™„ë£Œ ë° ì €ì¥!\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ì €ì¥\n",
    "import pickle\n",
    "results_3 = {\n",
    "    'eval_results': eval_results_3,\n",
    "    'model_config': {\n",
    "        'num_labels': num_labels,\n",
    "        'class_names': train_df['case_type'].cat.categories.tolist(),\n",
    "        'class_weights': class_weights.cpu().numpy()\n",
    "    },\n",
    "    'training_history': trainer_3.state.log_history,\n",
    "    'elapsed_time': elapsed_3\n",
    "}\n",
    "\n",
    "with open('../pkl_file/deep_data/train_deep_data_model3.pkl', 'wb') as f:\n",
    "    pickle.dump(results_3, f)\n",
    "\n",
    "print(\"âœ… MODE_3 ì™„ë£Œ ë° ì €ì¥!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "873fa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"ğŸƒ MODE 2: ë¹ ë¥¸ ê²€ì¦ (ì•½ 30ë¶„)\")\n",
    "# args = MODE_2_args\n",
    "# train_data = create_sample_dataset(train_dataset, 0.05)\n",
    "# sample_ratio = 0.05\n",
    "\n",
    "# elif mode == \"MODE_3\":\n",
    "# print(\"ğŸ“ MODE 3: ì‹¤ìš© ëª¨ë¸ (ì•½ 3ì‹œê°„)\")\n",
    "# args = MODE_3_args\n",
    "# train_data = create_sample_dataset(train_dataset, 0.10)\n",
    "# sample_ratio = 0.10\n",
    "# else:\n",
    "# raise ValueError(\"modeëŠ” 'MODE_1', 'MODE_2', 'MODE_3' ì¤‘ í•˜ë‚˜\")\n",
    "\n",
    "# print(\"=\"*70)\n",
    "\n",
    "\n",
    "# return trainer, eval_results\n",
    "\n",
    "# except Exception as e:\n",
    "# print(f\"\\nâŒ ì—ëŸ¬: {e}\")\n",
    "# raise \n",
    "\n",
    "# trainer, eval_results = run_training('MODE_1')  # ë˜ëŠ” MODE_2, MODE_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cad3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # í•™ìŠµ ì˜µì…˜ ì„¤ì •\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",            # ê²°ê³¼ ì €ì¥ í´ë”\n",
    "#     eval_strategy=\"steps\",             # í‰ê°€ ê¸°ì¤€ (steps)\n",
    "#     per_device_train_batch_size=4,    # í•œ ë²ˆì— 4ê°œì”©\n",
    "#     gradient_accumulation_steps=4,    # 4ê°œ ì‹¤í–‰í•´\n",
    "#     fp16=True,\n",
    "#     max_grad_norm=1.0,                 # ì•ˆì •ì„± í™•ë³´\n",
    "    \n",
    "#     num_train_epochs=3,                # ì „ì²´ ë°ì´í„° ë°˜ë³µ íšŸìˆ˜\n",
    "#     learning_rate=2e-5,                # í•™ìŠµë¥ \n",
    "    \n",
    "#     logging_steps=100,                 # 100ë‹¨ê³„ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥(ì˜ë˜ëŠ”ì§€ í™•ì¸ìš©ë„)\n",
    "#     save_steps=1000,                    # 1000ë‹¨ê³„ë§ˆë‹¤ ëª¨ë¸ ì €ì¥\n",
    "#     eval_steps=1000,                     # í‰ê°€ ë¹ˆë„\n",
    "#     save_total_limit=2,                  # ìµœì‹  2ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€\n",
    "    \n",
    "#     # ì„±ëŠ¥ ìµœì í™”\n",
    "#     dataloader_num_workers=2,            # ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”\n",
    "#     load_best_model_at_end=True,       # ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ ìµœì¢… ì„ íƒ\n",
    "#     metric_for_best_model=\"f1\",\n",
    "#     greater_is_better=True,\n",
    "\n",
    "#     report_to=\"none\",\n",
    "#     disable_tqdm=False,                  # ì§„í–‰ë¥  í‘œì‹œ\n",
    "# )\n",
    "# print(\"í•™ìŠµ í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6315313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ì„¤ì •\n",
    "# # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ ì „ ì‘ì€ ìƒ˜í”Œë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸\n",
    "# quick_test_args = TrainingArguments(\n",
    "#     output_dir=\"./quick_test\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     fp16=True,\n",
    "#     num_train_epochs=1,                  # 1 ì—í­ë§Œ\n",
    "#     max_steps=100,                       # 100 ìŠ¤í…ë§Œ ì‹¤í–‰\n",
    "#     learning_rate=2e-5,\n",
    "#     logging_steps=10,\n",
    "#     eval_steps=50,\n",
    "#     save_steps=50,\n",
    "#     report_to=\"none\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99abcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë°ì´í„° ìƒ˜í”Œë§ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "# # ì „ì²´ í•™ìŠµ ì „ 10%ë§Œ ì‚¬ìš©í•´ì„œ í…ŒìŠ¤íŠ¸\n",
    "# def create_sample_dataset(dataset, sample_ratio=0.1):\n",
    "#     \"\"\"ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ì¶”ì¶œ\"\"\"\n",
    "#     import random\n",
    "#     total_size = len(dataset)\n",
    "#     sample_size = int(total_size * sample_ratio)\n",
    "#     indices = random.sample(range(total_size), sample_size)\n",
    "    \n",
    "#     # ì„œë¸Œì…‹ ìƒì„±\n",
    "#     from torch.utils.data import Subset\n",
    "#     return Subset(dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbb1c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµê°€ë™\n",
    "# trainer = WeightedTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     compute_metrics=compute_metrics #ì±„ì ê¸°\n",
    "# )\n",
    "\n",
    "# print(\"ğŸš€ í•™ìŠµ ì‹œì‘\")\n",
    "# print(f\"ğŸ“Š ì´ í•™ìŠµ ë°ì´í„°: {len(train_dataset):,}ê±´\")\n",
    "# print(f\"ğŸ“Š ì´ í‰ê°€ ë°ì´í„°: {len(test_dataset):,}ê±´\")\n",
    "# print(f\"â±ï¸  ì˜ˆìƒ ì‹œê°„: ë°°ì¹˜í¬ê¸°ì™€ í•˜ë“œì›¨ì–´ì— ë”°ë¼ 3-10ì‹œê°„\")\n",
    "\n",
    "# # trainer.train()\n",
    "# # print(\" í•™ìŠµì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0f104ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # í•™ìŠµê²°ê³¼ í™•ì¸ ë° ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def predict_legal_case(text):\n",
    "#     # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
    "#     model.eval()\n",
    "    \n",
    "#     # ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    \n",
    "#     # ì˜ˆì¸¡\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "#         pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "#     # ê²°ê³¼ ì¶œë ¥\n",
    "#     class_names = train_df['case_type'].cat.categories.tolist()\n",
    "#     confidence = probs[0][pred_idx].item() * 100\n",
    "    \n",
    "#     print(f\"\\n[ì…ë ¥ ë¬¸ì¥]: {text[:50]}...\")\n",
    "#     print(f\"[ì˜ˆì¸¡ ê²°ê³¼]: {class_names[pred_idx]} ({confidence:.2f}% í™•ì‹ )\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "325a23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ›ï¸ [BERT ë²•ë¥  ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼]\n",
      "==================================================\n",
      "ğŸ“Š ì˜ˆìƒ ìŠ¹ì†Œìœ¨: 5.0%\n",
      "âš–ï¸ ì˜ˆìƒ í˜•ëŸ‰: 0.2ë…„\n",
      "âš ï¸ ìœ„í—˜ë„ ì ìˆ˜: 2.1/100\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ìœ—ì…€ ëŒ€ì‹  ì´ê±¸ë¡œ ë³€ê²½\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_full_analysis(text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)#.to(device)\n",
    "    \n",
    "    # token_type_ids ì œê±°\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items() if k != 'token_type_ids'}\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)#.to(device)\n",
    "    \n",
    "    # token_type_ids ì œê±°\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items() if k != 'token_type_ids'}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ›ï¸ [BERT ë²•ë¥  ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼]\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ğŸ“Š ì˜ˆìƒ ìŠ¹ì†Œìœ¨: {max(0, min(100, outputs['win_rate'].item())):.1f}%\")\n",
    "    \n",
    "    # í˜•ëŸ‰ì´ 0.1ë…„ ì´ìƒì¼ ë•Œë§Œ ì¶œë ¥\n",
    "    if outputs['sentence'].item() > 0.1:\n",
    "        print(f\"âš–ï¸ ì˜ˆìƒ í˜•ëŸ‰: {outputs['sentence'].item():.1f}ë…„\")\n",
    "    \n",
    "    # ë²Œê¸ˆì´ 1ë§Œì› ì´ìƒì¼ ë•Œë§Œ ì¶œë ¥\n",
    "    if outputs['fine'].item() > 10000:\n",
    "        print(f\"ğŸ’° ì˜ˆìƒ ë²Œê¸ˆ: {outputs['fine'].item():,.0f}ì›\")\n",
    "    \n",
    "    print(f\"âš ï¸ ìœ„í—˜ë„ ì ìˆ˜: {max(0, min(100, outputs['risk'].item())):.1f}/100\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "test_text = \"ì €ëŠ” íšŒì‚¬ì—ì„œ ë¶€ë‹¹í•´ê³ ë¥¼ ë‹¹í–ˆìŠµë‹ˆë‹¤. í‡´ì§ê¸ˆ 500ë§Œì›ë„ ëª» ë°›ì•˜ìŠµë‹ˆë‹¤.\"\n",
    "predict_full_analysis(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d689fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ğŸ” ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ---\n",
      "\n",
      "==================================================\n",
      "ğŸ›ï¸ [BERT ë²•ë¥  ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼]\n",
      "==================================================\n",
      "ğŸ“Š ì˜ˆìƒ ìŠ¹ì†Œìœ¨: 9.1%\n",
      "âš–ï¸ ì˜ˆìƒ í˜•ëŸ‰: 0.1ë…„\n",
      "âš ï¸ ìœ„í—˜ë„ ì ìˆ˜: 1.9/100\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ›ï¸ [BERT ë²•ë¥  ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼]\n",
      "==================================================\n",
      "ğŸ“Š ì˜ˆìƒ ìŠ¹ì†Œìœ¨: 9.0%\n",
      "âš ï¸ ìœ„í—˜ë„ ì ìˆ˜: 1.9/100\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---\n",
    "print(\"\\n--- ğŸ” ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ---\")\n",
    "sample_text = \"í”¼ê³ ì¸ì€ í”¼í•´ìì˜ ë¬¼ê±´ì„ ì ˆì·¨í•˜ì˜€ìœ¼ë©° ì´ë¥¼ ëª©ê²©í•œ ì¦ì¸ì´ ì¡´ì¬í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ë²”í–‰ì„ ë¶€ì¸í•˜ê³  ìˆë‹¤.\"\n",
    "predict_full_analysis(sample_text)\n",
    "\n",
    "sample_text_2 = \"ë³¸ ì¡°í•­ì€ êµ­ë¯¼ì˜ ê±°ì£¼ ì´ì „ì˜ ììœ ë¥¼ ê³¼ë„í•˜ê²Œ ì¹¨í•´í•˜ì—¬ í—Œë²•ì— ìœ„ë°°ë  ì†Œì§€ê°€ ìˆìŠµë‹ˆë‹¤.\"\n",
    "predict_full_analysis(sample_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90a5145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ë¯¼ì‚¬/ê°€ì‚¬ì†Œì†¡       0.00      0.00      0.00         8\n",
      "        í–‰ì •ì†Œì†¡       0.00      0.00      0.00        52\n",
      "        í˜•ì‚¬ì†Œì†¡       0.40      1.00      0.57        40\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.13      0.33      0.19       100\n",
      "weighted avg       0.16      0.40      0.23       100\n",
      "\n",
      "\n",
      "ğŸ¯ í˜¼ë™ í–‰ë ¬ (Confusion Matrix):\n",
      "[[ 0  0  8]\n",
      " [ 0  0 52]\n",
      " [ 0  0 40]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‹¤ì œ ì˜ˆì¸¡í•´ë³´ê¸°\n",
    "def evaluate_model_performance():\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import numpy as np\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(min(100, len(test_dataset))):  # 100ê°œ ìƒ˜í”Œë§Œ\n",
    "        item = test_dataset[i]\n",
    "        inputs = {k: v.unsqueeze(0).to(device) for k, v in item.items() \n",
    "                  if k in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = outputs['logits'].argmax(-1).item()\n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(item['labels'].item())\n",
    "    \n",
    "    # ìƒì„¸ ë¦¬í¬íŠ¸\n",
    "    class_names = train_df['case_type'].cat.categories.tolist()\n",
    "    print(\"\\nğŸ“Š ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    print(\"\\nğŸ¯ í˜¼ë™ í–‰ë ¬ (Confusion Matrix):\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# ì‹¤í–‰\n",
    "evaluate_model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f45d6023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305fb1b5dcda4af9a23bdf9bb024c831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:19<00:00,  1.36s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1fb8e3df3941ed97f18169c2ad0491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 48152 (\\N{HANGUL SYLLABLE BAN}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 48277 (\\N{HANGUL SYLLABLE BEOB}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 44033 (\\N{HANGUL SYLLABLE GAG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 53440 (\\N{HANGUL SYLLABLE TA}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 54805 (\\N{HANGUL SYLLABLE HYEONG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 49548 (\\N{HANGUL SYLLABLE SO}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 49569 (\\N{HANGUL SYLLABLE SONG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 54665 (\\N{HANGUL SYLLABLE HAENG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 48124 (\\N{HANGUL SYLLABLE MIN}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 44032 (\\N{HANGUL SYLLABLE GA}) missing from font(s) Arial.\n",
      "  plt.savefig(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fa8fdbae9d434bb137e23862d390da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6def5879971d47c6aa2cb66a0b1b1b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089348f36f7c4ba286e5600b9b88b6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.21it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b726a2d01fa487690506dcff5e90acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 44033 (\\N{HANGUL SYLLABLE GAG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 54805 (\\N{HANGUL SYLLABLE HYEONG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 49548 (\\N{HANGUL SYLLABLE SO}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 49569 (\\N{HANGUL SYLLABLE SONG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 54665 (\\N{HANGUL SYLLABLE HAENG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 48124 (\\N{HANGUL SYLLABLE MIN}) missing from font(s) Arial.\n",
      "  plt.savefig(\n",
      "c:\\Users\\human-24\\git\\Ai\\.venv\\lib\\site-packages\\ydata_profiling\\visualisation\\utils.py:73: UserWarning: Glyph 44032 (\\N{HANGUL SYLLABLE GA}) missing from font(s) Arial.\n",
      "  plt.savefig(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a9493b00f74c378c945175e9e4ce8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a55dc00ea734ec5a314ff3b0275a6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ! train_data_report.html íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ë°ì´í„° ë¦¬í¬íŠ¸\n",
    "train_report = ProfileReport(train_df, title=\"train_deep_Report\")\n",
    "train_report.to_file(\"../report/train_deep_Report.html\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¦¬í¬íŠ¸\n",
    "test_report = ProfileReport(test_df, title=\"Test_deep_Report\")\n",
    "test_report.to_file(\"../report/Test_deep_Report.html\")\n",
    "\n",
    "print(\"ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ! train_data_report.html íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
