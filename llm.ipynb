{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c882f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install langchain langchain-community langchain-text-splitters sentence-transformers faiss-cpu\n",
    "# ollama 설치 : https://ollama.com/download/windows\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # 문맥이 가능한 한 유지되도록 청크 분할(문단, 줄바꿈, 공백 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5a25199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드\n",
    "loader = TextLoader(\"./dataset/history.txt\", encoding='UTF8')  # 텍스트 파일 로드\n",
    "documents = loader.load()  # 문서 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a7cc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 벡터 임베딩 생성 (Hugging Face 사용)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e0abdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 검색기(retriever) 설정\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82ad7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ollama Gemma2 모델 초기화\n",
    "llm = Ollama(model=\"gemma2\", base_url=\"http://localhost:11434\")  # Ollama 서버 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b35d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG 체인 구성 (LCEL 방식)...\n"
     ]
    }
   ],
   "source": [
    "# 5. RAG 체인 구성 (LCEL 방식)\n",
    "print(\"\\nRAG 체인 구성 (LCEL 방식)...\")\n",
    "\n",
    "# 5.1. 프롬프트 템플릿 정의\n",
    "template = \"\"\"\n",
    "당신은 질문에 답변하는 AI 어시턴트입니다.\n",
    "제공된 context만을 바탕으로 질문에 답변하세요. 모르면 모른다고 답하세요.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 5.2. LCEL 체인 조합 (RetrievalQA 대체)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # 1. 검색\n",
    "    | prompt                                                   # 2. 프롬프트 조합\n",
    "    | llm                                                      # 3. LLM 호출\n",
    "    | StrOutputParser()                                        # 4. 출력 파싱\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c05b9a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG 질의 실행...\n",
      "\n",
      "[질문]: 고조선은 언제 설립되었는지 알려줘.\n",
      "[답변]: 기원전 2333년 단군왕검에 의해 세워졌다고 전해집니다.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. 질문 실행\n",
    "print(\"\\nRAG 질의 실행...\")\n",
    "query = \"고조선은 언제 설립되었는지 알려줘.\"\n",
    "try:\n",
    "    # invoke()를 사용하여 체인 실행\n",
    "    response = rag_chain.invoke(query)\n",
    "\n",
    "    print(\"\\n[질문]:\", query)\n",
    "    print(\"[답변]:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"RAG 체인 실행 중 오류: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59c2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\code\\.venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.6) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\code\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사전설치 : pip install langchain langchain-community chromadb sentence-transformers langchain-text-splitters\n",
    "import os\n",
    "import shutil  # 오래된 벡터 저장소를 삭제하기 위해 임포트\n",
    "# RAG 체인 구성에 필요한 핵심 모듈 임포트\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 나머지 모듈 임포트\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # 문맥이 가능한 한 유지되도록 청크 분할(문단, 줄바꿈, 공백 등)\n",
    "\n",
    "# 1. 설정\n",
    "# ----------------------------------------------------------------------\n",
    "# 로드할 .txt 파일 경로 (요청 경로 반영)\n",
    "TEXT_FILE_PATH = \"./dataset/history.txt\"\n",
    "# ChromaDB를 저장할 로컬 디렉토리 경로 (Windows 로컬 PC에 생성됨)\n",
    "PERSIST_DIRECTORY = \"./chroma_store\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL_NAME = \"gemma2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d72d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. './dataset/history.txt' 한글 파일 로드 중...\n",
      "-> 총 3개의 청크로 분할되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 2. 문서 로드 및 청크 분할\n",
    "print(f\"1. '{TEXT_FILE_PATH}' 한글 파일 로드 중...\")\n",
    "try:\n",
    "    loader = TextLoader(TEXT_FILE_PATH, encoding='UTF8')\n",
    "    documents = loader.load()\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 파일이 존재하지 않습니다. 경로를 확인하세요: {TEXT_FILE_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"파일 로드 중 오류 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 청크 크기를 늘려 더 많은 컨텍스트가 포함되도록 함\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"-> 총 {len(texts)}개의 청크로 분할되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee658eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. 임베딩 모델 정의\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFaceEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. 임베딩 모델 정의\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 한글에 특화된 HuggingFace 임베딩 모델을 로드합니다.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HuggingFaceEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. 임베딩 모델 정의\n",
    "print(f\"\\n2. 임베딩 모델 정의\")\n",
    "# 한글에 특화된 HuggingFace 임베딩 모델을 로드합니다.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ba108b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./chroma_store' (기존 벡터 저장소) 삭제 중...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: './chroma_store\\\\58a12857-0c80-4e34-be2e-499bb98cb686\\\\data_level0.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(PERSIST_DIRECTORY):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPERSIST_DIRECTORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (기존 벡터 저장소) 삭제 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPERSIST_DIRECTORY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. ChromaDB 벡터 저장소 생성 및 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPERSIST_DIRECTORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에 저장 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      9\u001b[0m     documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m     10\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m     11\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mPERSIST_DIRECTORY\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32mC:\\Python310\\lib\\shutil.py:749\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\shutil.py:614\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    612\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mislink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\shutil.py:619\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    617\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    618\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: './chroma_store\\\\58a12857-0c80-4e34-be2e-499bb98cb686\\\\data_level0.bin'"
     ]
    }
   ],
   "source": [
    "# 4. ChromaDB 벡터 저장소 생성\n",
    "# ----------------------------------------------------------------------\n",
    "if os.path.exists(PERSIST_DIRECTORY):\n",
    "    print(f\"'{PERSIST_DIRECTORY}' (기존 벡터 저장소) 삭제 중...\")\n",
    "    shutil.rmtree(PERSIST_DIRECTORY)\n",
    "\n",
    "print(f\"\\n3. ChromaDB 벡터 저장소 생성 및 '{PERSIST_DIRECTORY}'에 저장 중...\")\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    ")\n",
    "vectordb.persist()\n",
    "print(f\"-> 벡터 저장소가 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634bf102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Ollama LLM (gemma2) 및 검색기 초기화 중...\n",
      "-> Ollama LLM 연결 성공.\n"
     ]
    }
   ],
   "source": [
    "# 5. Ollama LLM 및 검색기(Retriever) 정의\n",
    "print(f\"\\n4. Ollama LLM ({OLLAMA_MODEL_NAME}) 및 검색기 초기화 중...\")\n",
    "\n",
    "try:\n",
    "    llm = Ollama(model=OLLAMA_MODEL_NAME, base_url=OLLAMA_BASE_URL)\n",
    "    llm.invoke(\"Hello\") # 연결 테스트\n",
    "    print(\"-> Ollama LLM 연결 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: Ollama LLM에 연결할 수 없습니다. (URL: {OLLAMA_BASE_URL})\")\n",
    "    print(f\"Ollama가 실행 중인지, '{OLLAMA_MODEL_NAME}' 모델이 설치되었는지 확인하세요.\")\n",
    "    exit()\n",
    "\n",
    "# 벡터 저장소를 검색기(Retriever)로 변환\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf1742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. RAG 체인 구성 (LCEL 방식)...\n"
     ]
    }
   ],
   "source": [
    "# 6. RAG 체인 구성 (LCEL 방식)\n",
    "print(\"\\n5. RAG 체인 구성 (LCEL 방식)...\")\n",
    "\n",
    "template = \"\"\"\n",
    "당신은 질문에 답변하는 AI 어시스턴트입니다.\n",
    "제공된 context만을 바탕으로 질문에 답변하세요. 모르면 모른다고 답하세요.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. RAG 질의 실행...\n",
      "\n",
      "[질문]: 고조선은 언제 설립되었는지 알려줘.\n",
      "[답변]: 고조선은 기원전 2333년에 설립되었다고 전해집니다.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. RAG 체인 실행 (질의)\n",
    "print(\"\\n6. RAG 질의 실행...\")\n",
    "query = \"고조선은 언제 설립되었는지 알려줘.\"\n",
    "\n",
    "try:\n",
    "    response = rag_chain.invoke(query)\n",
    "\n",
    "    print(\"\\n[질문]:\", query)\n",
    "    print(\"[답변]:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"RAG 체인 실행 중 오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77214e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 chroma 서버 구동 명령어(터미널) :  chroma run --host 0.0.0.0 --port 8000 --path ./chroma_store\n",
    "import time\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chromadb import HttpClient   # HttpClient: 원격 ChromaDB 서버와 통신하기 위한 클라이언트 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 중앙 벡터 DB 서버 정보 (관리자가 알려준 IP 입력)\n",
    "SERVER_HOST = \"localhost\"  # 실제 서버 IP로 변경 필요\n",
    "SERVER_PORT = 8000\n",
    "\n",
    "# 2. 로컬 LLM 설정 (각자 로컬에 설치된 Ollama 사용)\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"gemma2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[접속 시도] 중앙 벡터 서버 (localhost:8000) 연결 중...\n",
      "-> 서버 연결 성공!\n"
     ]
    }
   ],
   "source": [
    "# 1. 중앙 벡터 DB 서버 연결 테스트\n",
    "print(f\"[접속 시도] 중앙 벡터 서버 ({SERVER_HOST}:{SERVER_PORT}) 연결 중...\")\n",
    "try:\n",
    "    client = HttpClient(host=SERVER_HOST, port=SERVER_PORT)\n",
    "    client.heartbeat()  # 연결 확인\n",
    "    print(\"-> 서버 연결 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[오류] 서버 연결 실패. IP({SERVER_HOST})와 포트({SERVER_PORT})를 확인하세요.\\n{e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 임베딩 모델 정의 (벡터 DB 생성 시 사용한 모델과 동일해야 함)\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ed9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 원격 벡터 저장소 연결\n",
    "vector_store = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"langchain\",  # 최초 벡터저장소 collection_name 미지정시 \"langchain\" 이름의 컬렉션이 자동 저장됨\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "retriever = vector_store.as_retriever()\n",
    "print(\"벡터 저장소 연결 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74124606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LLM 및 RAG 체인 구성\n",
    "llm = Ollama(model=OLLAMA_MODEL, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "template = \"\"\"질문에 대해 제공된 Context만을 기반으로 답변하세요.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba49e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 질의 응답 인터랙티브 루프 (종료: exit)\n",
    "print(\"\\n[준비 완료] 질의를 시작합니다. (종료하려면 'exit' 입력)\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\n질문 입력 > \")\n",
    "    if query.lower() in [\"exit\", \"quit\", \"종료\"]:\n",
    "        print(\"질의 세션을 종료합니다.\")\n",
    "        break\n",
    "    if not query.strip():\n",
    "        continue\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"답변 생성 중...\", end=\"\", flush=True)\n",
    "    try:\n",
    "        response = rag_chain.invoke(query)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\r[답변] ({elapsed:.2f}초 소요)\\n{response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[오류 발생] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac28adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e226170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message_history = SQLChatMessageHistory(\n",
    "    session_id = \"sql_chat_history\",\n",
    "    connection_string = \"mysql+pymysql://root:123456@localhost:3306/test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09970bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message_history.add_user_message(\n",
    "    \"안녕. 나는 영준이야. 직업은 웹프로그래머이고 만나서 반가워!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83cb33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message_history.add_user_message(\n",
    "    \"요즘 날씨가 추운데 건강 조심하고 즐거운 하루 보내!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d0ce05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='안녕. 나는 영준이야. 직업은 웹프로그래머이고 만나서 반가워!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='요즘 날씨가 추운데 건강 조심하고 즐거운 하루 보내!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_message_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea6244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
